---->Two-output models:
# Define the input
input_tensor = Input(shape=(2,))
# Define the output
output_tensor = Dense(2)(input_tensor)
# Create a model
model = Model(input_tensor,output_tensor)
# Compile the model
model.compile(optimizer='adam',loss='mean_absolute_error')

# Fit the model
model.fit(games_tourney_train[['seed_diff', 'pred']],
  		  games_tourney_train[['score_1', 'score_2']],
  		  verbose=True,
  		  epochs=100,
  		  batch_size=16384)

# Print the model's weights
print(model.get_weights())
# Print the column means of the training data
print(games_tourney_train.mean())
# Evaluate the model on the tournament test data
print(model.evaluate(games_tourney_test[['seed_diff', 'pred']], games_tourney_test[['score_1', 'score_2']], verbose=False))

---->Single model for classification and regression:
# Create an input layer with 2 columns
input_tensor = Input(shape=(2,))
# Create the first output
output_tensor_1 = Dense(1, activation='linear', use_bias=False)(input_tensor)
# Create the second output (use the first output as input here)
output_tensor_2 = Dense(1, activation='sigmoid', use_bias=False)(output_tensor_1)
# Create a model with 2 outputs
model = Model(input_tensor, [output_tensor_1, output_tensor_2])

# Import the Adam optimizer
from tensorflow.keras.optimizers import Adam
# Compile the model with 2 losses and the Adam optimzer with a higher learning rate
model.compile(loss=['mean_absolute_error', 'binary_crossentropy'], optimizer=Adam(learning_rate=0.01))
# Fit the model to the tournament training data, with 2 inputs and 2 outputs
model.fit(games_tourney_train[['seed_diff','pred']],
          [games_tourney_train[['score_diff']], games_tourney_train[['won']]],
          epochs=10,
          verbose=True,
          batch_size=16384)

# Print the model weights
print(model.get_weights())
# Print the training data means
print(games_tourney_train.mean())
# Import the sigmoid function from scipy
from scipy.special import expit as sigmoid
# Weight from the model
weight = 0.14
# Print the approximate win probability predicted close game
print(sigmoid(1 * weight))
# Print the approximate win probability predicted blowout game
print(sigmoid(10 * weight))

'''
So sigmoid(1 * 0.14) is 0.53, which represents a pretty close game and sigmoid(10 * 0.14) is 0.80, which represents a pretty likely win. In other words, if the model predicts a win of 1 point, it is less sure of the win than if it predicts 10 points. Who says neural networks are black boxes?
'''
# Evaluate the model on new data
print(model.evaluate(games_tourney_test[['seed_diff','pred']],
               [games_tourney_test[['score_diff']], games_tourney_test[['won']]], verbose=False))